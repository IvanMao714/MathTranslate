% Code Completion : AST
@inproceedings{wang2021code,
  title={Code completion by modeling flattened abstract syntax trees as graphs},
  author={Wang, Yanlin and Li, Hui},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={16},
  pages={14015--14023},
  year={2021}
}

@article{wang2020towards,
  title={Towards Full-line Code Completion with Neural Language Models},
  author={Wang, Wenhan and Shen, Sijie and Li, Ge and Jin, Zhi},
  journal={arXiv preprint arXiv:2009.08603},
  year={2020}
}

@inproceedings{svyatkovskiy2019pythia,
  title={Pythia: Ai-assisted code completion system},
  author={Svyatkovskiy, Alexey and Zhao, Ying and Fu, Shengyu and Sundaresan, Neel},
  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2727--2735},
  year={2019}
}

@inproceedings{kim2021code,
  title={Code prediction by feeding trees to transformers},
  author={Kim, Seohyun and Zhao, Jinman and Tian, Yuchi and Chandra, Satish},
  booktitle={2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)},
  pages={150--162},
  year={2021},
  organization={IEEE}
}

@article{izadi2022codefill,
  title={CodeFill: Multi-token Code Completion by Jointly Learning from Structure and Naming Sequences},
  author={Izadi, Maliheh and Gismondi, Roberta and Gousios, Georgios},
  journal={arXiv preprint arXiv:2202.06689},
  year={2022}
}

@inproceedings{liu2020self,
  title={A self-attentional neural architecture for code completion with multi-task learning},
  author={Liu, Fang and Li, Ge and Wei, Bolin and Xia, Xin and Fu, Zhiyi and Jin, Zhi},
  booktitle={Proceedings of the 28th International Conference on Program Comprehension},
  pages={37--47},
  year={2020}
}

@article{liu2022unified,
  title={A unified multi-task learning model for AST-level and token-level code completion},
  author={Liu, Fang and Li, Ge and Wei, Bolin and Xia, Xin and Fu, Zhiyi and Jin, Zhi},
  journal={Empirical Software Engineering},
  volume={27},
  number={4},
  pages={1--38},
  year={2022},
  publisher={Springer}
}

% Code Completion : MTL
@inproceedings{liu2020multi,
  title={Multi-task learning based pre-trained language model for code completion},
  author={Liu, Fang and Li, Ge and Zhao, Yunfei and Jin, Zhi},
  booktitle={Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
  pages={473--485},
  year={2020}
}

% Code Completion : others modern
@article{li2017code,
  title={Code completion with neural attention and pointer networks},
  author={Li, Jian and Wang, Yue and Lyu, Michael R and King, Irwin},
  journal={arXiv preprint arXiv:1711.09573},
  year={2017}
}

@inproceedings{svyatkovskiy2020intellicode,
  title={Intellicode compose: Code generation using transformer},
  author={Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel},
  booktitle={Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1433--1443},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{brockschmidt2018generative,
  title={Generative code modeling with graphs},
  author={Brockschmidt, Marc and Allamanis, Miltiadis and Gaunt, Alexander L and Polozov, Oleksandr},
  journal={arXiv preprint arXiv:1805.08490},
  year={2018}
}

% Code Completion : others statistic model
@inproceedings{hou2010towards,
  title={Towards a better code completion system by API grouping, filtering, and popularity-based ranking},
  author={Hou, Daqing and Pletcher, David M},
  booktitle={Proceedings of the 2nd International Workshop on Recommendation Systems for Software Engineering},
  pages={26--30},
  year={2010}
}

@inproceedings{bruch2009learning,
  title={Learning from examples to improve code completion systems},
  author={Bruch, Marcel and Monperrus, Martin and Mezini, Mira},
  booktitle={Proceedings of the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium on the foundations of software engineering},
  pages={213--222},
  year={2009}
}

@inproceedings{robbes2008program,
  title={How program history can improve code completion},
  author={Robbes, Romain and Lanza, Michele},
  booktitle={2008 23rd IEEE/ACM International Conference on Automated Software Engineering},
  pages={317--326},
  year={2008},
  organization={IEEE}
}

@inproceedings{hellendoorn2017deep,
  title={Are deep neural networks the best choice for modeling source code?},
  author={Hellendoorn, Vincent J and Devanbu, Premkumar},
  booktitle={Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
  pages={763--773},
  year={2017}
}

@article{raychev2016probabilistic,
  title={Probabilistic model for code with decision trees},
  author={Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
  journal={ACM SIGPLAN Notices},
  volume={51},
  number={10},
  pages={731--747},
  year={2016},
  publisher={ACM New York, NY, USA}
}

% Decoding Methods
@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}

@article{ackley1985learning,
  title={A learning algorithm for Boltzmann machines},
  author={Ackley, David H and Hinton, Geoffrey E and Sejnowski, Terrence J},
  journal={Cognitive science},
  volume={9},
  number={1},
  pages={147--169},
  year={1985},
  publisher={Elsevier}
}

% Decoding Methods: Beam search
@inproceedings{li-etal-2016-deep,
    title = "Deep Reinforcement Learning for Dialogue Generation",
    author = "Li, Jiwei  and
      Monroe, Will  and
      Ritter, Alan  and
      Jurafsky, Dan  and
      Galley, Michel  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1127",
    doi = "10.18653/v1/D16-1127",
    pages = "1192--1202",
}

@inproceedings{wiseman-etal-2017-challenges,
    title = "Challenges in Data-to-Document Generation",
    author = "Wiseman, Sam  and
      Shieber, Stuart  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1239",
    doi = "10.18653/v1/D17-1239",
    pages = "2253--2263",
    abstract = "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.",
}

% others
@article{ruder2017overview,
  title={An overview of multi-task learning in deep neural networks},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1706.05098},
  year={2017}
}

@article{weller2022use,
  title={When to Use Multi-Task Learning vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning},
  author={Weller, Orion and Seppi, Kevin and Gardner, Matt},
  journal={arXiv preprint arXiv:2205.08124},
  year={2022}
}

@article{gururangan2020don,
  title={Don't stop pretraining: adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}

@article{lin2021closer,
  title={A closer look at loss weighting in multi-task learning},
  author={Lin, Baijiong and Ye, Feiyang and Zhang, Yu},
  journal={arXiv preprint arXiv:2111.10603},
  year={2021}
}

@inproceedings{10.5555/2337223.2337322,
author = {Hindle, Abram and Barr, Earl T. and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
title = {On the Naturalness of Software},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {837â€“847},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@article{hindle2016naturalness,
  title={On the naturalness of software},
  author={Hindle, Abram and Barr, Earl T and Gabel, Mark and Su, Zhendong and Devanbu, Premkumar},
  journal={Communications of the ACM},
  volume={59},
  number={5},
  pages={122--131},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{
lu2021codexglue,
title={Code{XGLUE}: A Machine Learning Benchmark Dataset for Code Understanding and Generation},
author={Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and MING GONG and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie LIU},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
year={2021},
url={https://openreview.net/forum?id=6lE4dQXaUcb}
}

@article{husain2019codesearchnet,
  title={Codesearchnet challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}

@article{phang2018sentence,
  title={Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks},
  author={Phang, Jason and F{\'e}vry, Thibault and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1811.01088},
  year={2018}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{wang2021codet5,
  title={Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2109.00859},
  year={2021}
}

@article{feng2020codebert,
  title={Codebert: A pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  journal={arXiv preprint arXiv:2002.08155},
  year={2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{kaushik2021understanding,
  title={Understanding catastrophic forgetting and remembering in continual learning with optimal relevance mapping},
  author={Kaushik, Prakhar and Gain, Alex and Kortylewski, Adam and Yuille, Alan},
  journal={arXiv preprint arXiv:2102.11343},
  year={2021}
}

% website
@online{intelliSense,
  author = {Visual Studio Code},
  title = {IntelliSense in Visual Studio Code},
  year = {2022},
  url = {https://code.visualstudio.com/docs/editor/intellisense},
  urldate = {2022-04-08}
}

@online{ml2022google,
  author = {Maxim Tabachnyk, Staff Software Engineer and Stoyan Nikolov, Senior Engineering Manager, Google Research},
  title = {ML-Enhanced Code Completion Improves Developer Productivity},
  year = 2022,
  url = {https://ai.googleblog.com/2022/07/ml-enhanced-code-completion-improves.html},
  urldate = {2022-06-26}
}

% BPE
@article{gage1994new,
  title={A new algorithm for data compression},
  author={Gage, Philip},
  journal={C Users Journal},
  volume={12},
  number={2},
  pages={23--38},
  year={1994},
  publisher={McPherson, KS: R \& D Publications, c1987-1994.}
}

@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}

@ARTICLE{1966SPhD...10..707L,
       author = {{Levenshtein}, V.~I.},
        title = "{Binary Codes Capable of Correcting Deletions, Insertions and Reversals}",
      journal = {Soviet Physics Doklady},
         year = 1966,
        month = feb,
       volume = {10},
        pages = {707},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1966SPhD...10..707L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{vandenhende2021multi,
  title={Multi-task learning for dense prediction tasks: A survey},
  author={Vandenhende, Simon and Georgoulis, Stamatios and Van Gansbeke, Wouter and Proesmans, Marc and Dai, Dengxin and Van Gool, Luc},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2021},
  publisher={IEEE}
}

@article{sener2018multi,
  title={Multi-task learning as multi-objective optimization},
  author={Sener, Ozan and Koltun, Vladlen},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier}
}

% mtl in SE

@INPROCEEDINGS{8882756,  author={Gong, Xi and Xing, Zhenchang and Li, Xiaohong and Feng, Zhiyong and Han, Zhuobing},  booktitle={2019 24th International Conference on Engineering of Complex Computer Systems (ICECCS)},   title={Joint Prediction of Multiple Vulnerability Characteristics Through Multi-Task Learning},   year={2019},  volume={},  number={},  pages={31-40},  doi={10.1109/ICECCS.2019.00011}}

@article{aung2022multi,
  title={Multi-triage: A multi-task learning framework for bug triage},
  author={Aung, Thazin Win Win and Wan, Yao and Huo, Huan and Sui, Yulei},
  journal={Journal of Systems and Software},
  volume={184},
  pages={111133},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{she2020mtfuzz,
  title={MTFuzz: fuzzing with a multi-task neural network},
  author={She, Dongdong and Krishna, Rahul and Yan, Lu and Jana, Suman and Ray, Baishakhi},
  booktitle={Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={737--749},
  year={2020}
}

@inproceedings{le2021deepcva,
  title={Deepcva: Automated commit-level vulnerability assessment with deep multi-task learning},
  author={Le, Triet Huynh Minh and Hin, David and Croft, Roland and Babar, M Ali},
  booktitle={2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  pages={717--729},
  year={2021},
  organization={IEEE}
}

@inproceedings{karampatsis2020big,
  title={Big code!= big vocabulary: Open-vocabulary models for source code},
  author={Karampatsis, Rafael-Michael and Babii, Hlib and Robbes, Romain and Sutton, Charles and Janes, Andrea},
  booktitle={2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)},
  pages={1073--1085},
  year={2020},
  organization={IEEE}
}

@article{fu2022gpt2sp,
  title={GPT2SP: A Transformer-Based Agile Story Point Estimation Approach},
  author={Fu, Michael and Tantithamthavorn, Chakkrit},
  journal={IEEE Transactions on Software Engineering},
  year={2022},
  publisher={IEEE}
}

@article{thongtanunam2022autotransform,
  title={AutoTransform: Automated Code Transformation to Support Modern Code Review Process},
  author={Thongtanunam, Patanamon and Pornprasit, Chanathip and Tantithamthavorn, Chakkrit},
  year={2022}
}