% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{intelliSense}
\BIBentryALTinterwordspacing
V.~S. Code. (2022) Intellisense in visual studio code. [Online]. Available:
  \url{https://code.visualstudio.com/docs/editor/intellisense}
\BIBentrySTDinterwordspacing

\bibitem{ml2022google}
\BIBentryALTinterwordspacing
S.~S.~E. Maxim~Tabachnyk and G.~R. Stoyan~Nikolov, Senior Engineering~Manager.
  (2022) Ml-enhanced code completion improves developer productivity. [Online].
  Available:
  \url{https://ai.googleblog.com/2022/07/ml-enhanced-code-completion-improves.html}
\BIBentrySTDinterwordspacing

\bibitem{lu2021codexglue}
\BIBentryALTinterwordspacing
S.~Lu, D.~Guo, S.~Ren, J.~Huang, A.~Svyatkovskiy, A.~Blanco, C.~Clement,
  D.~Drain, D.~Jiang, D.~Tang, G.~Li, L.~Zhou, L.~Shou, L.~Zhou, M.~Tufano,
  M.~GONG, M.~Zhou, N.~Duan, N.~Sundaresan, S.~K. Deng, S.~Fu, and S.~LIU,
  ``Code{XGLUE}: A machine learning benchmark dataset for code understanding
  and generation,'' in \emph{Thirty-fifth Conference on Neural Information
  Processing Systems Datasets and Benchmarks Track (Round 1)}, 2021. [Online].
  Available: \url{https://openreview.net/forum?id=6lE4dQXaUcb}
\BIBentrySTDinterwordspacing

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever \emph{et~al.},
  ``Language models are unsupervised multitask learners,'' \emph{OpenAI blog},
  vol.~1, no.~8, p.~9, 2019.

\bibitem{svyatkovskiy2020intellicode}
A.~Svyatkovskiy, S.~K. Deng, S.~Fu, and N.~Sundaresan, ``Intellicode compose:
  Code generation using transformer,'' in \emph{Proceedings of the 28th ACM
  Joint Meeting on European Software Engineering Conference and Symposium on
  the Foundations of Software Engineering}, 2020, pp. 1433--1443.

\bibitem{kim2021code}
S.~Kim, J.~Zhao, Y.~Tian, and S.~Chandra, ``Code prediction by feeding trees to
  transformers,'' in \emph{2021 IEEE/ACM 43rd International Conference on
  Software Engineering (ICSE)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2021, pp. 150--162.

\bibitem{izadi2022codefill}
M.~Izadi, R.~Gismondi, and G.~Gousios, ``Codefill: Multi-token code completion
  by jointly learning from structure and naming sequences,'' \emph{arXiv
  preprint arXiv:2202.06689}, 2022.

\bibitem{brockschmidt2018generative}
M.~Brockschmidt, M.~Allamanis, A.~L. Gaunt, and O.~Polozov, ``Generative code
  modeling with graphs,'' \emph{arXiv preprint arXiv:1805.08490}, 2018.

\bibitem{li2017code}
J.~Li, Y.~Wang, M.~R. Lyu, and I.~King, ``Code completion with neural attention
  and pointer networks,'' \emph{arXiv preprint arXiv:1711.09573}, 2017.

\bibitem{svyatkovskiy2019pythia}
A.~Svyatkovskiy, Y.~Zhao, S.~Fu, and N.~Sundaresan, ``Pythia: Ai-assisted code
  completion system,'' in \emph{Proceedings of the 25th ACM SIGKDD
  International Conference on Knowledge Discovery \& Data Mining}, 2019, pp.
  2727--2735.

\bibitem{liu2020self}
F.~Liu, G.~Li, B.~Wei, X.~Xia, Z.~Fu, and Z.~Jin, ``A self-attentional neural
  architecture for code completion with multi-task learning,'' in
  \emph{Proceedings of the 28th International Conference on Program
  Comprehension}, 2020, pp. 37--47.

\bibitem{liu2022unified}
------, ``A unified multi-task learning model for ast-level and token-level
  code completion,'' \emph{Empirical Software Engineering}, vol.~27, no.~4, pp.
  1--38, 2022.

\bibitem{raychev2016probabilistic}
V.~Raychev, P.~Bielik, and M.~Vechev, ``Probabilistic model for code with
  decision trees,'' \emph{ACM SIGPLAN Notices}, vol.~51, no.~10, pp. 731--747,
  2016.

\bibitem{hou2010towards}
D.~Hou and D.~M. Pletcher, ``Towards a better code completion system by api
  grouping, filtering, and popularity-based ranking,'' in \emph{Proceedings of
  the 2nd International Workshop on Recommendation Systems for Software
  Engineering}, 2010, pp. 26--30.

\bibitem{robbes2008program}
R.~Robbes and M.~Lanza, ``How program history can improve code completion,'' in
  \emph{2008 23rd IEEE/ACM International Conference on Automated Software
  Engineering}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2008, pp.
  317--326.

\bibitem{bruch2009learning}
M.~Bruch, M.~Monperrus, and M.~Mezini, ``Learning from examples to improve code
  completion systems,'' in \emph{Proceedings of the 7th joint meeting of the
  European software engineering conference and the ACM SIGSOFT symposium on the
  foundations of software engineering}, 2009, pp. 213--222.

\bibitem{10.5555/2337223.2337322}
A.~Hindle, E.~T. Barr, Z.~Su, M.~Gabel, and P.~Devanbu, ``On the naturalness of
  software,'' in \emph{Proceedings of the 34th International Conference on
  Software Engineering}, ser. ICSE '12.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE Press, 2012, p. 837â€“847.

\bibitem{hindle2016naturalness}
A.~Hindle, E.~T. Barr, M.~Gabel, Z.~Su, and P.~Devanbu, ``On the naturalness of
  software,'' \emph{Communications of the ACM}, vol.~59, no.~5, pp. 122--131,
  2016.

\bibitem{wang2021code}
Y.~Wang and H.~Li, ``Code completion by modeling flattened abstract syntax
  trees as graphs,'' in \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, vol.~35, no.~16, 2021, pp. 14\,015--14\,023.

\bibitem{sennrich2015neural}
R.~Sennrich, B.~Haddow, and A.~Birch, ``Neural machine translation of rare
  words with subword units,'' \emph{arXiv preprint arXiv:1508.07909}, 2015.

\bibitem{fu2022gpt2sp}
M.~Fu and C.~Tantithamthavorn, ``Gpt2sp: A transformer-based agile story point
  estimation approach,'' \emph{IEEE Transactions on Software Engineering},
  2022.

\bibitem{karampatsis2020big}
R.-M. Karampatsis, H.~Babii, R.~Robbes, C.~Sutton, and A.~Janes, ``Big code!=
  big vocabulary: Open-vocabulary models for source code,'' in \emph{2020
  IEEE/ACM 42nd International Conference on Software Engineering (ICSE)}.\hskip
  1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 1073--1085.

\bibitem{thongtanunam2022autotransform}
P.~Thongtanunam, C.~Pornprasit, and C.~Tantithamthavorn, ``Autotransform:
  Automated code transformation to support modern code review process,'' 2022.

\bibitem{phang2018sentence}
J.~Phang, T.~F{\'e}vry, and S.~R. Bowman, ``Sentence encoders on stilts:
  Supplementary training on intermediate labeled-data tasks,'' \emph{arXiv
  preprint arXiv:1811.01088}, 2018.

\bibitem{ruder2017overview}
S.~Ruder, ``An overview of multi-task learning in deep neural networks,''
  \emph{arXiv preprint arXiv:1706.05098}, 2017.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{husain2019codesearchnet}
H.~Husain, H.-H. Wu, T.~Gazit, M.~Allamanis, and M.~Brockschmidt,
  ``Codesearchnet challenge: Evaluating the state of semantic code search,''
  \emph{arXiv preprint arXiv:1909.09436}, 2019.

\bibitem{holtzman2019curious}
A.~Holtzman, J.~Buys, L.~Du, M.~Forbes, and Y.~Choi, ``The curious case of
  neural text degeneration,'' \emph{arXiv preprint arXiv:1904.09751}, 2019.

\bibitem{li-etal-2016-deep}
\BIBentryALTinterwordspacing
J.~Li, W.~Monroe, A.~Ritter, D.~Jurafsky, M.~Galley, and J.~Gao, ``Deep
  reinforcement learning for dialogue generation,'' in \emph{Proceedings of the
  2016 Conference on Empirical Methods in Natural Language Processing}.\hskip
  1em plus 0.5em minus 0.4em\relax Austin, Texas: Association for Computational
  Linguistics, Nov. 2016, pp. 1192--1202. [Online]. Available:
  \url{https://aclanthology.org/D16-1127}
\BIBentrySTDinterwordspacing

\bibitem{wiseman-etal-2017-challenges}
\BIBentryALTinterwordspacing
S.~Wiseman, S.~Shieber, and A.~Rush, ``Challenges in data-to-document
  generation,'' in \emph{Proceedings of the 2017 Conference on Empirical
  Methods in Natural Language Processing}.\hskip 1em plus 0.5em minus
  0.4em\relax Copenhagen, Denmark: Association for Computational Linguistics,
  Sep. 2017, pp. 2253--2263. [Online]. Available:
  \url{https://aclanthology.org/D17-1239}
\BIBentrySTDinterwordspacing

\bibitem{ackley1985learning}
D.~H. Ackley, G.~E. Hinton, and T.~J. Sejnowski, ``A learning algorithm for
  boltzmann machines,'' \emph{Cognitive science}, vol.~9, no.~1, pp. 147--169,
  1985.

\bibitem{vandenhende2021multi}
S.~Vandenhende, S.~Georgoulis, W.~Van~Gansbeke, M.~Proesmans, D.~Dai, and
  L.~Van~Gool, ``Multi-task learning for dense prediction tasks: A survey,''
  \emph{IEEE transactions on pattern analysis and machine intelligence}, 2021.

\bibitem{sener2018multi}
O.~Sener and V.~Koltun, ``Multi-task learning as multi-objective
  optimization,'' \emph{Advances in neural information processing systems},
  vol.~31, 2018.

\bibitem{wang2020towards}
W.~Wang, S.~Shen, G.~Li, and Z.~Jin, ``Towards full-line code completion with
  neural language models,'' \emph{arXiv preprint arXiv:2009.08603}, 2020.

\bibitem{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga \emph{et~al.}, ``Pytorch: An imperative
  style, high-performance deep learning library,'' \emph{Advances in neural
  information processing systems}, vol.~32, 2019.

\bibitem{wolf2019huggingface}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz \emph{et~al.}, ``Huggingface's transformers:
  State-of-the-art natural language processing,'' \emph{arXiv preprint
  arXiv:1910.03771}, 2019.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''
  \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{1966SPhD...10..707L}
V.~I. {Levenshtein}, ``{Binary Codes Capable of Correcting Deletions,
  Insertions and Reversals},'' \emph{Soviet Physics Doklady}, vol.~10, p. 707,
  Feb. 1966.

\end{thebibliography}
