
% *** TITLE/SUBJECT/AUTHOR/KEYWORDS INFO BELOW!!           ***
\documentclass[10pt,journal,compsoc]{IEEEtran}
\usepackage{mypreamble}
\usepackage{relsize}
\usepackage[inline]{enumitem}
\newcommand\kla[1]{{\textcolor{blue}{Kla: #1}}}
\newcommand\gam[1]{{\textcolor{orange}{Gam: #1}}}
\newcommand\yf[1]{{\textcolor{brown}{Yuan-Fang: #1}}}
\newcommand\our{PyCoder}
\newcommand\ea[1]{\emph{et al.~}}
\usepackage{tcolorbox}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=0.5pt] (char) {\small{#1}};}}
\usepackage{amsmath, nccmath}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

% RQ 
\newcommand\rqone{What is the performance of our \our~for the token-level and line-level code completion tasks when compared to state-of-the-art models?}
\newcommand\rqtwo{What is the impact of the training strategies on the performance of our \our?}
\newcommand\rqthree{What is the impact of the task weighting parameters in multi-task learning on the performance of our \our?}
\newcommand\rqfour{What is the impact of the decoding methods on the performance of our \our?}

% findings
\newcommand\fdone{
\our~achieves the first rank on the CodeXGLUE leaderboard with an accuracy of 77.12\% for the token-level predictions, which is 0.43\%-24.25\% more accurate than  baselines.
In addition, \our~achieves an exact match of 43.37\% for the line-level predictions, which is 3.63\%-84.73\% more accurate than baselines.
% \our~surpasses all the state-of-the-art models, i.e.\ Pointer Mixture Network~\cite{li2017code}, TravTrans~\cite{kim2021code}, GPT-2~\cite{radford2019language}, and CodeGPT~\cite{lu2021codexglue}, which includes both AST and non-AST approaches.
% The line-level exact match is improved by 8.34\%-15.22\%, while the token-level accuracy is improved by 1.89\%-11.74\%.
% \our~also receives the first place in CodeXGLUE’s python code completion benchmark. The results indicate that the token type syntactic information can be beneficial for code completion.
}
\newcommand\fdtwo{
Multi-task training strategies have an impact on \our~for both token-level and line-level predictions.
We find that \our-Hard performs best; followed by \our-IFN and \our-Soft.
% The different multi-task training strategies have an impact on the performance of \our~for both token-level and line-level predictions.
% Particularly, the exact match in line-level prediction is vary by 4.54\%, and the accuracy in token-level prediction is vary by 2.23\%.
% The best multi-task training technique (section~\ref{sec:approach-arch}) for our setting is MTL: Hard Parameter Sharing; followed by IFN: Intermediate Fine-Tuning and MTL: Soft Parameter Sharing.
}
\newcommand\fdthree{\our~is generally robust to the task weighting parameters, achieving comparative (without task weighting) or better (with task weighting) performance when compared to the baselines.}
% For \our, the best task weighting parameter (type:code) is 1:9, meaning that with the minimum weighting of 10\% for the token type prediction task, \our~already outperforms state-of-the-art.}
\newcommand\fdfour{Decoding methods have an impact on the performance of \our~with an exact match varying from 33.80\% to 41.52\% for line-level predictions. 
Beam Search performs best, while Sampling performs worst.
% In addition, we find that not only the methods but different libraries with different implementations also produce different results.
% In our setting, Beam Search(CodeXGLUE) performs the best, while Sampling performs the worst.
}

\newcommand\MYhyperrefoptions{
bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={black},citecolor={black},urlcolor={black},
pdftitle={Syntactic-Aware On-the-Fly Code Completion},
pdfsubject={PyCoder},%<!CHANGE!
pdfauthor={Wannita Takerngsaksiri},
pdfkeywords={Code Completion, Multi-Task Learning, STILTs, Syntactic Information, Transformers}}
% \renewcommand*\footnoterule\\{}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\input{myacronyms.tex}

\input{sections/titleAbstract}

% \linenumbers

\input{sections/introduction}

\input{sections/background}

\input{sections/approach}

\input{sections/experiments}

\input{sections/results}

\input{sections/discussion}

% \input{sections/relatedwork.tex}

\input{sections/threatToValidity}

\input{sections/conclusion}

\section*{Acknowledgment}
Chakkrit Tantithamthavorn was partly supported by the Australian Research Council’s Discovery Early Career Researcher Award (DECRA) funding scheme (DE200100941).

\bibliography{mybibfile}

\appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.


% use section* for acknowledgment
% \ifCLASSOPTIONcompsoc
%   % The Computer Society usually uses the plural form
%   \section*{Acknowledgments}
% \else
%   % regular IEEE prefers the singular form
%   \section*{Acknowledgment}
% \fi



\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\bibliographystyle{IEEEtran}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{profile/Wannita.jpg}}]{Wannita Takerngsaksiri}
is a Ph.D. candidate at Monash University, Australia. Her research interest includes code generation, machine learning (ML), and natural language processing (NLP).
Specifically, her research goal aims to develop computational methods and AI techniques to assist the coding process to be easier for developers and novice programmers in practice. 
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{profile/kla.jpg}}]{Chakkrit (Kla) Tantithamthavorn}
is an ARC DECRA Fellow and a Senior Lecturer in Software Engineering in the Faculty of Information Technology, Monash University, Australia. He is pioneering an emerging research area of Explainable AI for Software Engineering (http://xai4se.github.io), inventing many AI-based technologies to improve developers’ productivity and make software systems more reliable and more secure, while being explainable to practitioners. To date, the XAI4SE book has attracted 10,000+ page views from 70 countries worldwide. He regularly published at TSE, ICSE, FSE, EMSE, ASE, and MSR, all of which are top software engineering venues. The excellence of his research is recognized through many awards including an ACM SIGSOFT Distinguished Paper Award 2021, an ARC’s Discovery Early Career Researcher Award 2020, the World Most Impactful Early-Stage SE Researcher based on a bibliometric assessment of software engineering (2013-2020). 
\end{IEEEbiography}

% if you will not have a photo at all:
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{profile/yuan-fang.jpeg}}]{Yuan-Fang Li}
is an Associate Professor at Faculty of IT, Monash University. His research interest is artificial intelligence, particularly the intersection between natural language processing and knowledge representation. His recent investigations include the following tasks: (1) neuro-symbolic approaches to complex question answering, (2) knowledge graph construction from text/images, and (3) graph representation learning. His research work has been published at top AI and NLP venues including ACL, EMNLP, ECCV, ICCV, ICLR, NeurIPS, IEEE TNNLS, and Pattern Recognition. 
\end{IEEEbiography}

\end{document}


